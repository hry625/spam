{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sTsDfIVKsmL"
   },
   "source": [
    "# DAT565 Introduction to Data Science and AI \n",
    "## 2023-2024, LP2\n",
    "## Assignment 4: Spam classification using Naïve Bayes \n",
    "This assignment has three obligatory questions. Questions 4-5 are optional and will not be graded.\n",
    "\n",
    "The exercise takes place in this notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n",
    "\n",
    "*Tips:* \n",
    "* You can execute certain Linux shell commands by prefixing the command with a `!`. \n",
    "* You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results, the second you can use to write code snippets that execute the tasks required.  \n",
    "\n",
    "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \n",
    "\n",
    "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n",
    "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n",
    "-\thard-ham: non-spam messages more difficult to differentiate \n",
    "-\tspam: spam messages \n",
    "\n",
    "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** \n",
    "\n",
    "If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer. Note that if you are using Windows, you can instead use [7zip](https://www.7-zip.org/download.html) to decompress the data (you will have to modify the cell below).\n",
    "\n",
    "**What to submit:** \n",
    "* Convert the notebook to a PDF file by compiling it, and submit the PDF file. \n",
    "* Make sure all cells are executed so all your code and its results are included. \n",
    "* Double-check that the PDF displays correctly before you submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wa37fBwRF-xe"
   },
   "outputs": [],
   "source": [
    "# download and extract the data\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
    "!tar -xjf 20021010_easy_ham.tar.bz2\n",
    "!tar -xjf 20021010_hard_ham.tar.bz2\n",
    "!tar -xjf 20021010_spam.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdH1XTepLjZ3"
   },
   "source": [
    "The data is now in the following three folders: `easy_ham`, `hard_ham`, and `spam`. You can confirm this via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A53Gw00fBLG2"
   },
   "outputs": [],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlWPVnSNzT7"
   },
   "source": [
    "### 1. Preprocessing: \n",
    "Note that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text (in the optional part further down, you can experiment with filtering out the headers and footers). \n",
    "1.\tWe don’t want to train and test on the same data (it might help to reflect on **why** ,if you don't recall). Split the spam and ham datasets into a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`). Use `easy_ham` for quesions 1 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J2sllUWXKblD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1785\n",
      "766\n",
      "350\n",
      "151\n",
      "['0439.2c6d4d3e8b817c5315a1c61e4eeb7ee9', '0372.216f90ef52558ed24402e192586a40e8', '0280.2507969221ea95a019506366f6c361d8', '0011.2a1247254a535bac29c476b86c708901', '0099.c4ff6dba0a5177d3c7d8ef54c8920496', '0131.0b7281078874ca88f95d6fdf5d905d50', '0423.3b9b432fa25a7ac8c494db8d2e42cc73', '0199.955edee89f34960c033c4d1072841356', '0091.113ec7122d4046a2754bcf70b9fb5299', '0338.033c0109da096486c7d797cccd2c3198', '0326.80f15e07265a22b78068bab5b56b01c7', '0166.a2e4d6ec3078b619ca38927ca69fc94d', '0203.beb1b157fc74672074061434cc7bad3c', '0197.6968d98720065059247cefe4e5bcd192', '0301.ad155a30cca1f9d16e75e8934030edae', '0051.374f4d4300a5d39544b2f052e7a9429d', '0204.33e3cd4e0ad791304e554bb259bda53e', '0319.e4a20802d12937998f3b3bf805362a3f', '0415.e241b6184464107168656739bf96c6b9', '0335.9822e1787fca0741a8501bdef7e8bc79', '0080.77af9ca7f967f055062aade45001129e', '0270.d50e186af7a00114ad967b8f77b70338', '0244.7e5d917c8a76d52cc694c5cf8ab8497d', '0047.376bd7728ee94b32bc23429d9c51bae5', '0049.625bab436c7fc6299cfceeaa24e198ae', '0433.8977506bae8028f48290ea0fb2f54ddd', '0024.fc4bd0b22cd7907e99f8a35b74655b15', '0312.a0e7f2633bd0ceaddf16fba58be54778', '0156.279e5f92cf12922fbbf0cbda112b7fcb', '0149.3300ef4537e1f6accd4489125bef5b0d', '0067.02c6e51107f39ee60453ff8e7372101d', '0385.8db8e827e6fec2fae5f7e407fe0e0ca3', '0265.1120a7d868b23e83b91ad00ec8b79e08', '0277.4b6ea2e210cacce0e46064da22a05549', '0477.c825fdd16a32d2d29f4f3613e412dfb3', '0295.717c906cda92746007b9098e16c727b2', '0322.77dd826a00ebd4b54a6036394d41da55', '0006.7a32642f8c22bbeb85d6c3b5f3890a2c', '0397.c02eba1386b00d640c954e5117dd1aa0', '0097.dce08392ba6bc552d13394fa73974b62', '0215.57c4f4d8e2f582088f8aca38239059f7', '0243.458c8e32e405b69f561fd77bc16f440c', '0447.badf14ca8ec589b0b2d25369573ffe84', '0043.8d93819b95ff90bf2e2b141c2909bfc9', '0124.37afd066a74d18b7f14bea0b1fb43d4d', '0096.b2cb600e893f7a663ea5f9bff3a6276e', '0001.bfc8d64d12b325ff385cca8d07b84288', '0307.2e4dc0cdb1e3b49f0986c19c1f324224', '0258.1d61b380a23168881253ed86bb4f79ac', '0112.ec411d26d1f4decc16af7ef73e69a227', '0254.02daa37a4255a78f2f224f3cd2f8fa99', '0154.e39fc51ffdb9c2ecd480ce972078aeaa', '0454.943161603b1eb6da5187194b5a75186a', '0500.2e8762b67913d1b07bc8da293448d27f', '0359.2794a4ec8f226ea59a009e972d012f64', '0151.6f8f0ec4d897a5285d662ef4ec31d924', '0370.6caa3885378bbaf0856bd9712629ab51', '0381.492ed1e5eed1b631560e2009be5b8c9a', '0360.5f5fc66c831d845705efac502121308a', '0342.babb5045c49b585808041391599bc05d', '0456.578afb400f87833c03c4dc2be6fd85bd', '0209.59817ef0dc8d05d4b49bd5914fa88afa', '0121.772c3ccd1b6c1a2e0e2ec0356082c77b', '0416.112b010a30255d7d14ee9465d4fe804c', '0465.5d27c24cf1797e87a346c02f9bc42587', '0362.d605ea00a259c1245d6e21ecf38264cf', '0002.24b47bb3ce90708ae29d0aec1da08610', '0239.43b3279a300a122610f91725bb92a538', '0100.c60d1c697136b07c947fa180ba3e0441', '0125.44381546181fc6c5d7ea59e917f232c5', '0076.770f0e7b8378a47a945043434f6f43df', '0093.2bb8a2a7e4d2841a14f27f32076dd77e', '0389.ed4ca8aceef91808c783909351c7bdb4', '0159.8a5c778f65ecc30e14507369b9eb8292', '0153.eddc658b08a04641a2494ba6b6eb0a3c', '0037.7ce3307b56dd90453027a6630179282e', '0445.2cd5092859b75fdccf3724ab2a4ccafe', '0130.e258624171c813fc6057728c0ff0c059', '0375.ad5939ae436ed745d5222893d5ffe191', '0348.e0b89978fa806cf3e7fd3ba0869b3c65', '0388.23ff533336b63fb45d267b8cbe59b7b4', '0211.195957199f6e9f694f9811ad83eda5c4', '0189.3d89383221aa3fb155a099838ce9c40a', '0117.33011fddf61efe5f453a14468ff7e629', '0402.1290489e7e62ac9bb500677606540e5d', '0072.f97a14d667569ebbc0502bb2c7beec27', '0432.a2fa136962969f603b363e7509668b49', '0314.5b03e0718373f3319eadaec592308aba', '0450.a828f09ee44e716e6931866e4743e32a', '0414.4b85e87c5b9235c72f189bf044057f15', '0495.a13bce4369913c929a48b073f2b320c9', '0472.40695f88feb07754e40b1008aeb47bd1', '0205.d3c294d833fd7c79edd96dac71039821', '0474.81ea7ec9e00168efe6bb824d08825ef3', '0259.7ebf3c0fd752bce5b8056e9454d2c76f', '0221.ee1d208001fd30265827fb309441d662', '0233.e9834d55f8185a84ce8a047b2eba2139', '0462.2cfe24a32d1b480eab70b099b4a8a919', '0255.42a6feb4435a0a68929075c0926f085d', '0334.3e4946e69031f3860ac6de3d3f27aadd', '0282.b9f0c6ac87b24a9abac5f2a564c0a6c6', '0155.829bab9379cfe32fe4b5af15ca99361b', '0210.285d263b1a18e67c68ec9fe005253dd0', '0247.aee6d7984b3dab9a6b0eed524e7a3686', '0458.8c9d2363547ebaf997282910cf21e95c', '0129.78a705ff6b3bde3395d067459e6e46e2', '0226.409b6577c79d85773d50cb37fde4ba79', '0223.8ab642208d33d7f9ac50bc2e42c02732', '0371.e4ca4edd7d69a9b54cac7f364f5f2079', '0476.0530ef1b33305fdf919489946570685b', '0488.6d41f6d7222978a3ee2b6cfbfce55a02', '0222.6ad799703d958681d6e427762f86f179', '0174.3874b6ff3c86a5ebefb558138a6bfb28', '0483.488a059db4a731caae8143bf298d6ac4', '0241.abb2882a304357a47681f887244c2f76', '0300.fa3ece84a195f3d36a70f2550824071f', '0246.3b997087302d48ff57ab5afb3d400d5b', '0315.26ca39910895a935e2b8bca93a44ebfe', '0470.b9e513715695ea1c79c1e5af0fb0eea9', '0286.68b939e20e5b9fd6839471f6e9bc07ed', '0298.804507b6d4d03a86e53c63249fe70772', '0383.5b89d5a9c0152070a77e133734f7cd83', '0176.70022adaab1a9dfe64ae7588ffa5add9', '0297.9e6095368b4e8258e967798cea8fe40e', '0084.df5ac85de3405b6d07c9fa7ba3eecf6a', '0141.516a4fe92f63469bd4a21d46dd6bb3be', '0056.0f99dde58a7c4e18944397ab47e0412c', '0467.b59a3337d0979ba8d587bf4c166db8b1', '0367.c6d1767b20048ef840cf83f8fb2cff68', '0014.ed99ffe0f452b91be11684cbfe8d349c', '0238.7d0de37650a0c0e2d99e52eef4042602', '0184.a2109736d2f15cde3747a6f335c6f24c', '0340.8e191c37e2d30a639013203aacf60086', '0318.da63a8488410932cf780238ec0ba59eb', '0253.f715f442da45114754198a160195b883', '0088.f421d8c380fb0c48483f026d243df9d9', '0377.31267c80e042b22be0436c044c13513a', '0008.9562918b57e044abfbce260cc875acde', '0010.7f5fb525755c45eb78efc18d7c9ea5aa', '0059.a633106e3ce62fa7b46c2e4dc8c666d3', '0355.94ebf637e4bd3db8a81c8ce68ecf681d', '0455.b7a7254a180821d6077a42f7153e12e5', '0170.fe4f77fa9456b48dffa9288074b2bb2a', '0424.b283405aeac6a8ff4c4a4e4e85310268', '0408.87f7a3c9c29aaf97b413126029aacc6a', '0463.47a4c19eea5230ff19a42e62a5f59484', '0005.1f42bb885de0ef7fc5cd09d34dc2ba54', '0394.9c882c72ddfd810b56776fdaa1c727a6', '0146.6656452972931e859e640f6ac57d2962', '0366.539843bed9a06ae77966ccbc9dc2e103', '0140.a2bb669eaf743ed123fca884a40cfbd4']\n"
     ]
    }
   ],
   "source": [
    "# write your pre-processing code here\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import collections\n",
    "hamFileList= os.listdir(\"./easy_ham\")\n",
    "# collections.shuffle(hamFileList)\n",
    "spamFileList= os.listdir(\"./spam\")\n",
    "hamtrain,hamtest = train_test_split(hamFileList,test_size=0.3)\n",
    "print(len(hamtrain))\n",
    "\n",
    "print(len(hamtest))\n",
    "\n",
    "spamtrain, spamtest = train_test_split(spamFileList,test_size=0.3)\n",
    "print(len(spamtrain))\n",
    "\n",
    "print(len(spamtest))\n",
    "print(spamtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbrbI0_OKCF"
   },
   "source": [
    "### 2. Write a Python program that: \n",
    "1.\tUses the four datasets from Question 1 (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`).\n",
    "2.\tTrains a Naïve Bayes classifier (use the [scikit-learn library](https://scikit-learn.org/stable/)) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)) to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifiers available in *scikit-learn* ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Here, you will test two of these classifiers that are well suited for this problem:\n",
    "- Multinomial Naive Bayes\n",
    "- Bernoulli Naive Bayes.\n",
    "\n",
    "Please inspect the documentation to ensure input to the classifiers is appropriate before you start coding. You may have to modify your input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def extractFeatureFromFile(filePathList:[],isSpam:bool):\n",
    "    result= pd.DataFrame()\n",
    "    vectorizer = CountVectorizer(lowercase=True , decode_error=\"ignore\")\n",
    "    for filePath in filePathList:\n",
    "        if isSpam:\n",
    "            file= open(\"./spam/\"+filePath, \"r\")\n",
    "        else:\n",
    "            file = open(\"./easy_ham/\",+filePath, \"r\")\n",
    "            \n",
    "        X = vectorizer.fit_transform(file)\n",
    "        new_row = pd.DataFrame({\"feature\":[X],\"isSpam\":1 if isSpam else 0})\n",
    "        result = result.append(new_row)\n",
    "    return result\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def extractFeatureFromFile1(filePathList:[],isSpam:bool):\n",
    "    result= pd.DataFrame()\n",
    "    for filePath in filePathList:\n",
    "        if isSpam:\n",
    "            file= open(\"./spam/\"+filePath, \"rb\")\n",
    "        else:\n",
    "            file = open(\"./easy_ham/\",+filePath, \"rb\")\n",
    "            \n",
    "        new_row = pd.DataFrame({\"feature\":file,\"isSpam\":1 if isSpam else 0})\n",
    "        result = result.append(new_row)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "def extractFeatureFromFile2(filePathList, isSpam):\n",
    "    result = pd.DataFrame()\n",
    "    all_text = []\n",
    "\n",
    "    for filePath in filePathList:\n",
    "        if isSpam:\n",
    "            file_path = \"./spam/\" + filePath\n",
    "        else:\n",
    "            file_path = \"./easy_ham/\" + filePath\n",
    "\n",
    "        # with open(file_path, \"r\",encoding='latin-1') as file:\n",
    "        #     data = file.read()\n",
    "        #     data = data.replace('\\n', ' ')\n",
    "        #     all_text.append(data)\n",
    "        with open(file_path, \"r\",encoding='iso-8859-1') as file:\n",
    "            data = file.read()\n",
    "            data = data.replace('\\n', ' ')\n",
    "            all_text.append(data)\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2)).fit(all_text)\n",
    "\n",
    "    for text in all_text:\n",
    "        X = vectorizer.transform([text])\n",
    "        new_row = pd.DataFrame({\"feature\": [X], \"isSpam\": 1 if isSpam else 0})\n",
    "        result = result.append(new_row, ignore_index=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MJERHSCcGNaW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/z_773gfs2d3cd8ch6mq3_s3r0000gn/T/ipykernel_984/572207603.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './easy_ham/0439.2c6d4d3e8b817c5315a1c61e4eeb7ee9'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/flyf/Downloads/assignment-4-1.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m spamtrain_df \u001b[39m=\u001b[39m extractFeatureFromFile2(spamtrain,\u001b[39mTrue\u001b[39;00m) \u001b[39m#[features,1]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m hamtrain_df \u001b[39m=\u001b[39m extractFeatureFromFile2(hamtrain,\u001b[39mFalse\u001b[39;00m)  \u001b[39m#[features,0]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m spamtest_df \u001b[39m=\u001b[39m extractFeatureFromFile2(spamtest,\u001b[39mFalse\u001b[39;49;00m)  \u001b[39m#[features,0]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m hamtest_df \u001b[39m=\u001b[39m extractFeatureFromFile2(hamtest,\u001b[39mFalse\u001b[39;00m)  \u001b[39m#[features,0]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_df\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([spamtrain_df,hamtrain_df], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/Users/flyf/Downloads/assignment-4-1.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./easy_ham/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m filePath\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# with open(file_path, \"r\",encoding='latin-1') as file:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#     data = file.read()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#     data = data.replace('\\n', ' ')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#     all_text.append(data)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m,encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39miso-8859-1\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     data \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flyf/Downloads/assignment-4-1.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './easy_ham/0439.2c6d4d3e8b817c5315a1c61e4eeb7ee9'"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "spamtrain_df = extractFeatureFromFile2(spamtrain,True) #[features,1]\n",
    "hamtrain_df = extractFeatureFromFile2(hamtrain,False)  #[features,0]\n",
    "spamtest_df = extractFeatureFromFile2(spamtest,False)  #[features,0]\n",
    "hamtest_df = extractFeatureFromFile2(hamtest,False)  #[features,0]\n",
    "train_df= pd.concat([spamtrain_df,hamtrain_df], ignore_index=True)\n",
    "test_df= pd.concat([spamtest_df,hamtest_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "multinomial_classifier = MultinomialNB()\n",
    "multinomial_classifier.fit(train_df[\"feature\"], train_df[\"isSpam\"])\n",
    "\n",
    "bernoulli_classifier = BernoulliNB()\n",
    "bernoulli_classifier.fit(train_df[\"feature\"], train_df[\"isSpam\"])\n",
    "\n",
    "y_pred_multinomial = multinomial_classifier.predict(test_df[\"feature\"])\n",
    "y_pred_bernoulli = bernoulli_classifier.predict(test_df[\"feature\"])\n",
    "\n",
    "\n",
    "bernoulli_classifier_confusion = confusion_matrix(test_df[\"isSpam\"], y_pred_bernoulli, labels=['ham', 'spam'])\n",
    "bernoulli_classifier_true_positive_rate = bernoulli_classifier_confusion[0, 0] / (bernoulli_classifier_confusion[0, 0] + bernoulli_classifier_confusion[0, 1])\n",
    "bernoulli_classifier_false_negative_rate = bernoulli_classifier_confusion[1, 0] / (bernoulli_classifier_confusion[1, 0] + bernoulli_classifier_confusion[1, 1])\n",
    "\n",
    "multinomial_classifier_confusion = confusion_matrix(test_df[\"isSpam\"], y_pred_multinomial, labels=['ham', 'spam'])\n",
    "multinomial_classifier_true_positive_rate = multinomial_classifier_confusion[0, 0] / (multinomial_classifier_confusion[0, 0] + multinomial_classifier_confusion[0, 1])\n",
    "multinomial_classifier_false_negative_rate = multinomial_classifier_confusion[1, 0] / (multinomial_classifier_confusion[1, 0] + multinomial_classifier_confusion[1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDFS3uFFUcS7"
   },
   "source": [
    "### 3. Run on hard ham:\n",
    "Run the two models from Question 2 on `spam` versus `hard-ham`, and compare to the `easy-ham` results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gool_zb8Qzzy"
   },
   "outputs": [],
   "source": [
    "# code to report results here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkfQWBB4UhYd"
   },
   "source": [
    "### 4.\tOPTIONAL - NOT MARKED: \n",
    "To avoid classification based on common and uninformative words, it is common practice to filter these out. \n",
    "\n",
    "**a.** Think about why this may be useful. Show a few examples of too common and too uncommon words. \n",
    "\n",
    "**b.** Use the parameters in *scikit-learn*’s `CountVectorizer` to filter out these words. Update the program from Question 2 and run it on `easy-ham` vs `spam` and `hard-ham` vs `spam`. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qt7ELzEqUfas"
   },
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcyVfOZFU4F_"
   },
   "source": [
    "### 5. OPTIONAL - NOT MARKED: Further improving performance\n",
    "Filter out the headers and footers of the emails before you run on them. The format may vary somewhat between emails, which can make this a bit tricky, so perfect filtering is not required. Run your program again and answer the following questions: \n",
    "- Does the result improve from those obtained in Questions 3 and 4? \n",
    "- What do you expect would happen if your training set consisted mostly of spam messages, while your test set consisted mostly of ham messages, or vice versa? \n",
    "- Look at the `fit_prior` parameter. What does this parameter mean? Discuss in what settings it can be helpful (you can also test your hypothesis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkIB6h9k4r07"
   },
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
